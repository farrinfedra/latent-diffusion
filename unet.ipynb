{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d4b2aed-5ec1-4e25-ae49-65089c8fb1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import importlib\n",
    "import torch\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2115c1bf-da37-4d40-b561-f28541f4e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_from_config(config):\n",
    "    if not \"target\" in config:\n",
    "        raise KeyError(\"Expected key `target` to instantiate.\")\n",
    "    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n",
    "\n",
    "def get_obj_from_str(string, reload=False):\n",
    "    module, cls = string.rsplit(\".\", 1)\n",
    "    if reload:\n",
    "        module_imp = importlib.import_module(module)\n",
    "        importlib.reload(module_imp)\n",
    "    return getattr(importlib.import_module(module, package=None), cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ec873e9-5646-4f00-b1e9-78cbbb79a629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "modulator is False\n",
      "*****Using Uformer-B******\n",
      "DiffusionWrapper has 51.83 M params.\n",
      "Keeping EMAs of 738.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Restored from models/first_stage_models/kl-f8/model.ckpt\n",
      "checkpoint path is models/first_stage_models/kl-f8/model.ckpt\n",
      "Training LatentDiffusion as an unconditional model.\n"
     ]
    }
   ],
   "source": [
    "cfg_path = \"/kuacc/users/bbiner21/Github/latent-diffusion/configs/latent-diffusion/lsun_churches-ldm-kl-8.yaml\"\n",
    "\n",
    "config = OmegaConf.load(cfg_path) \n",
    "\n",
    "model = instantiate_from_config(config.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7202c60e-33e6-4c39-b95f-4e0e37b4c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55451a39-d387-4217-97f5-696c7e93e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(4,4,32,32)\n",
    "num_timesteps = 1000\n",
    "t = torch.randint(0, num_timesteps, (x.shape[0],)).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c856b16-d4a5-4c2f-bb19-f83dfcb4294e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input x shape is torch.Size([4, 4, 32, 32])\n",
      "after input proj y shape is torch.Size([4, 1024, 32])\n",
      "after dropout proj y shape is torch.Size([4, 1024, 32])\n",
      "query shape is torch.Size([64, 1, 64, 32])\n",
      "key shape is torch.Size([64, 1, 64, 32])\n",
      "attn shape is torch.Size([64, 1, 64, 64])\n",
      "pool0 shape is torch.Size([4, 256, 64])\n",
      "query shape is torch.Size([16, 2, 64, 32])\n",
      "key shape is torch.Size([16, 2, 64, 32])\n",
      "attn shape is torch.Size([16, 2, 64, 64])\n",
      "query shape is torch.Size([16, 2, 64, 32])\n",
      "key shape is torch.Size([16, 2, 64, 32])\n",
      "attn shape is torch.Size([16, 2, 64, 64])\n",
      "pool1 shape is torch.Size([4, 64, 128])\n",
      "query shape is torch.Size([4, 4, 64, 32])\n",
      "key shape is torch.Size([4, 4, 64, 32])\n",
      "attn shape is torch.Size([4, 4, 64, 64])\n",
      "query shape is torch.Size([4, 4, 64, 32])\n",
      "key shape is torch.Size([4, 4, 64, 32])\n",
      "attn shape is torch.Size([4, 4, 64, 64])\n",
      "query shape is torch.Size([4, 4, 64, 32])\n",
      "key shape is torch.Size([4, 4, 64, 32])\n",
      "attn shape is torch.Size([4, 4, 64, 64])\n",
      "query shape is torch.Size([4, 4, 64, 32])\n",
      "key shape is torch.Size([4, 4, 64, 32])\n",
      "attn shape is torch.Size([4, 4, 64, 64])\n",
      "query shape is torch.Size([4, 4, 64, 32])\n",
      "key shape is torch.Size([4, 4, 64, 32])\n",
      "attn shape is torch.Size([4, 4, 64, 64])\n",
      "query shape is torch.Size([4, 4, 64, 32])\n",
      "key shape is torch.Size([4, 4, 64, 32])\n",
      "attn shape is torch.Size([4, 4, 64, 64])\n",
      "query shape is torch.Size([4, 4, 64, 32])\n",
      "key shape is torch.Size([4, 4, 64, 32])\n",
      "attn shape is torch.Size([4, 4, 64, 64])\n",
      "query shape is torch.Size([4, 4, 64, 32])\n",
      "key shape is torch.Size([4, 4, 64, 32])\n",
      "attn shape is torch.Size([4, 4, 64, 64])\n",
      "pool2 shape is torch.Size([4, 16, 256])\n",
      "query shape is torch.Size([4, 8, 16, 32])\n",
      "key shape is torch.Size([4, 8, 16, 32])\n",
      "attn shape is torch.Size([4, 8, 16, 16])\n",
      "query shape is torch.Size([4, 8, 16, 32])\n",
      "key shape is torch.Size([4, 8, 16, 32])\n",
      "attn shape is torch.Size([4, 8, 16, 16])\n",
      "query shape is torch.Size([4, 8, 16, 32])\n",
      "key shape is torch.Size([4, 8, 16, 32])\n",
      "attn shape is torch.Size([4, 8, 16, 16])\n",
      "query shape is torch.Size([4, 8, 16, 32])\n",
      "key shape is torch.Size([4, 8, 16, 32])\n",
      "attn shape is torch.Size([4, 8, 16, 16])\n",
      "query shape is torch.Size([4, 8, 16, 32])\n",
      "key shape is torch.Size([4, 8, 16, 32])\n",
      "attn shape is torch.Size([4, 8, 16, 16])\n",
      "query shape is torch.Size([4, 8, 16, 32])\n",
      "key shape is torch.Size([4, 8, 16, 32])\n",
      "attn shape is torch.Size([4, 8, 16, 16])\n",
      "query shape is torch.Size([4, 8, 16, 32])\n",
      "key shape is torch.Size([4, 8, 16, 32])\n",
      "attn shape is torch.Size([4, 8, 16, 16])\n",
      "query shape is torch.Size([4, 8, 16, 32])\n",
      "key shape is torch.Size([4, 8, 16, 32])\n",
      "attn shape is torch.Size([4, 8, 16, 16])\n",
      "pool3 shape is torch.Size([4, 4, 512])\n",
      "query shape is torch.Size([4, 16, 4, 32])\n",
      "key shape is torch.Size([4, 16, 4, 32])\n",
      "attn shape is torch.Size([4, 16, 4, 4])\n",
      "query shape is torch.Size([4, 16, 4, 32])\n",
      "key shape is torch.Size([4, 16, 4, 32])\n",
      "attn shape is torch.Size([4, 16, 4, 4])\n",
      "conv4 shape is torch.Size([4, 4, 512])\n",
      "query shape is torch.Size([4, 16, 16, 32])\n",
      "key shape is torch.Size([4, 16, 16, 32])\n",
      "attn shape is torch.Size([4, 16, 16, 16])\n",
      "query shape is torch.Size([4, 16, 16, 32])\n",
      "key shape is torch.Size([4, 16, 16, 32])\n",
      "attn shape is torch.Size([4, 16, 16, 16])\n",
      "query shape is torch.Size([4, 16, 16, 32])\n",
      "key shape is torch.Size([4, 16, 16, 32])\n",
      "attn shape is torch.Size([4, 16, 16, 16])\n",
      "query shape is torch.Size([4, 16, 16, 32])\n",
      "key shape is torch.Size([4, 16, 16, 32])\n",
      "attn shape is torch.Size([4, 16, 16, 16])\n",
      "query shape is torch.Size([4, 16, 16, 32])\n",
      "key shape is torch.Size([4, 16, 16, 32])\n",
      "attn shape is torch.Size([4, 16, 16, 16])\n",
      "query shape is torch.Size([4, 16, 16, 32])\n",
      "key shape is torch.Size([4, 16, 16, 32])\n",
      "attn shape is torch.Size([4, 16, 16, 16])\n",
      "query shape is torch.Size([4, 16, 16, 32])\n",
      "key shape is torch.Size([4, 16, 16, 32])\n",
      "attn shape is torch.Size([4, 16, 16, 16])\n",
      "query shape is torch.Size([4, 16, 16, 32])\n",
      "key shape is torch.Size([4, 16, 16, 32])\n",
      "attn shape is torch.Size([4, 16, 16, 16])\n",
      "deconv0 shape is torch.Size([4, 16, 512])\n",
      "query shape is torch.Size([4, 8, 64, 32])\n",
      "key shape is torch.Size([4, 8, 64, 32])\n",
      "attn shape is torch.Size([4, 8, 64, 64])\n",
      "query shape is torch.Size([4, 8, 64, 32])\n",
      "key shape is torch.Size([4, 8, 64, 32])\n",
      "attn shape is torch.Size([4, 8, 64, 64])\n",
      "query shape is torch.Size([4, 8, 64, 32])\n",
      "key shape is torch.Size([4, 8, 64, 32])\n",
      "attn shape is torch.Size([4, 8, 64, 64])\n",
      "query shape is torch.Size([4, 8, 64, 32])\n",
      "key shape is torch.Size([4, 8, 64, 32])\n",
      "attn shape is torch.Size([4, 8, 64, 64])\n",
      "query shape is torch.Size([4, 8, 64, 32])\n",
      "key shape is torch.Size([4, 8, 64, 32])\n",
      "attn shape is torch.Size([4, 8, 64, 64])\n",
      "query shape is torch.Size([4, 8, 64, 32])\n",
      "key shape is torch.Size([4, 8, 64, 32])\n",
      "attn shape is torch.Size([4, 8, 64, 64])\n",
      "query shape is torch.Size([4, 8, 64, 32])\n",
      "key shape is torch.Size([4, 8, 64, 32])\n",
      "attn shape is torch.Size([4, 8, 64, 64])\n",
      "query shape is torch.Size([4, 8, 64, 32])\n",
      "key shape is torch.Size([4, 8, 64, 32])\n",
      "attn shape is torch.Size([4, 8, 64, 64])\n",
      "deconv1 shape is torch.Size([4, 64, 256])\n",
      "query shape is torch.Size([16, 4, 64, 32])\n",
      "key shape is torch.Size([16, 4, 64, 32])\n",
      "attn shape is torch.Size([16, 4, 64, 64])\n",
      "query shape is torch.Size([16, 4, 64, 32])\n",
      "key shape is torch.Size([16, 4, 64, 32])\n",
      "attn shape is torch.Size([16, 4, 64, 64])\n",
      "deconv2 shape is torch.Size([4, 256, 128])\n",
      "query shape is torch.Size([64, 2, 64, 32])\n",
      "key shape is torch.Size([64, 2, 64, 32])\n",
      "attn shape is torch.Size([64, 2, 64, 64])\n",
      "deconv3 shape is torch.Size([4, 1024, 64])\n",
      "y shape is torch.Size([4, 4, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 32, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.model.diffusion_model(x,t)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "295e7c2a-c6f7-4859-b759-dc467f5c48ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eace7ea4-3001-43ae-a4da-ecca70b8658e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/scratch/users/bbiner21/Github/latent-diffusion', '/kuacc/users/bbiner21/.conda/envs/taming/lib/python38.zip', '/kuacc/users/bbiner21/.conda/envs/taming/lib/python3.8', '/kuacc/users/bbiner21/.conda/envs/taming/lib/python3.8/lib-dynload', '', '/kuacc/users/bbiner21/.local/lib/python3.8/site-packages', '/kuacc/users/bbiner21/.conda/envs/taming/lib/python3.8/site-packages', '/scratch/users/bbiner21/Github/latent-diffusion', '/kuacc/users/bbiner21/Github/Uformer']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60872664-ee8c-450e-9a5e-6f3a9ec2e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/kuacc/users/bbiner21/Github/Uformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cf31e82-ad71-489c-8802-53a0c5f94a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Uformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76d6fd52-3324-4564-a830-f3d9c8ca60d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'val/loss_simple_ema'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4aaaaaaa-c761-439c-b6f7-1be769ec289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_restoration = Uformer(img_size=128,embed_dim=16,win_size=8,token_projection='linear',token_mlp='leff',modulator=True,dd_in=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61a96cc7-aee7-4542-a9c8-ff844ffd4127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Uformer(\n",
       "  embed_dim=16, token_projection=linear, token_mlp=leff,win_size=8\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (input_proj): InputProj(\n",
       "    (proj): Sequential(\n",
       "      (0): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (output_proj): OutputProj(\n",
       "    (proj): Sequential(\n",
       "      (0): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (encoderlayer_0): BasicUformerLayer(\n",
       "    dim=16, input_resolution=(128, 128), depth=2\n",
       "    (blocks): ModuleList(\n",
       "      (0): LeWinTransformerBlock(\n",
       "        dim=16, input_resolution=(128, 128), num_heads=1, win_size=8, shift_size=0, mlp_ratio=4.0,modulator=None\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): WindowAttention(\n",
       "          dim=16, win_size=(8, 8), num_heads=1\n",
       "          (qkv): LinearProjection(\n",
       "            (to_q): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (to_kv): Linear(in_features=16, out_features=32, bias=True)\n",
       "          )\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): LeFF(\n",
       "          (linear1): Sequential(\n",
       "            (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (dwconv): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (linear2): Sequential(\n",
       "            (0): Linear(in_features=64, out_features=16, bias=True)\n",
       "          )\n",
       "          (eca): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): LeWinTransformerBlock(\n",
       "        dim=16, input_resolution=(128, 128), num_heads=1, win_size=8, shift_size=4, mlp_ratio=4.0,modulator=None\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): WindowAttention(\n",
       "          dim=16, win_size=(8, 8), num_heads=1\n",
       "          (qkv): LinearProjection(\n",
       "            (to_q): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (to_kv): Linear(in_features=16, out_features=32, bias=True)\n",
       "          )\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): LeFF(\n",
       "          (linear1): Sequential(\n",
       "            (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (dwconv): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (linear2): Sequential(\n",
       "            (0): Linear(in_features=64, out_features=16, bias=True)\n",
       "          )\n",
       "          (eca): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dowsample_0): Downsample(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (encoderlayer_1): BasicUformerLayer(\n",
       "    dim=32, input_resolution=(64, 64), depth=2\n",
       "    (blocks): ModuleList(\n",
       "      (0): LeWinTransformerBlock(\n",
       "        dim=32, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0,modulator=None\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): WindowAttention(\n",
       "          dim=32, win_size=(8, 8), num_heads=2\n",
       "          (qkv): LinearProjection(\n",
       "            (to_q): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (to_kv): Linear(in_features=32, out_features=64, bias=True)\n",
       "          )\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): LeFF(\n",
       "          (linear1): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (dwconv): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (linear2): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "          (eca): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): LeWinTransformerBlock(\n",
       "        dim=32, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0,modulator=None\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): WindowAttention(\n",
       "          dim=32, win_size=(8, 8), num_heads=2\n",
       "          (qkv): LinearProjection(\n",
       "            (to_q): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (to_kv): Linear(in_features=32, out_features=64, bias=True)\n",
       "          )\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): LeFF(\n",
       "          (linear1): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (dwconv): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (linear2): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "          (eca): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dowsample_1): Downsample(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (encoderlayer_2): BasicUformerLayer(\n",
       "    dim=64, input_resolution=(32, 32), depth=2\n",
       "    (blocks): ModuleList(\n",
       "      (0): LeWinTransformerBlock(\n",
       "        dim=64, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0,modulator=None\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): WindowAttention(\n",
       "          dim=64, win_size=(8, 8), num_heads=4\n",
       "          (qkv): LinearProjection(\n",
       "            (to_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (to_kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "          )\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): LeFF(\n",
       "          (linear1): Sequential(\n",
       "            (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (dwconv): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (linear2): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "          )\n",
       "          (eca): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): LeWinTransformerBlock(\n",
       "        dim=64, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0,modulator=None\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): WindowAttention(\n",
       "          dim=64, win_size=(8, 8), num_heads=4\n",
       "          (qkv): LinearProjection(\n",
       "            (to_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (to_kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "          )\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): LeFF(\n",
       "          (linear1): Sequential(\n",
       "            (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (dwconv): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (linear2): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "          )\n",
       "          (eca): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dowsample_2): Downsample(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (encoderlayer_3): BasicUformerLayer(\n",
       "    dim=128, input_resolution=(16, 16), depth=2\n",
       "    (blocks): ModuleList(\n",
       "      (0): LeWinTransformerBlock(\n",
       "        dim=128, input_resolution=(16, 16), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0,modulator=None\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): WindowAttention(\n",
       "          dim=128, win_size=(8, 8), num_heads=8\n",
       "          (qkv): LinearProjection(\n",
       "            (to_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (to_kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): LeFF(\n",
       "          (linear1): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (dwconv): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (linear2): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (eca): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): LeWinTransformerBlock(\n",
       "        dim=128, input_resolution=(16, 16), num_heads=8, win_size=8, shift_size=4, mlp_ratio=4.0,modulator=None\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): WindowAttention(\n",
       "          dim=128, win_size=(8, 8), num_heads=8\n",
       "          (qkv): LinearProjection(\n",
       "            (to_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (to_kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): LeFF(\n",
       "          (linear1): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (dwconv): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (linear2): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (eca): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dowsample_3): Downsample(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (conv): BasicUformerLayer(\n",
       "    dim=256, input_resolution=(8, 8), depth=2\n",
       "    (blocks): ModuleList(\n",
       "      (0): LeWinTransformerBlock(\n",
       "        dim=256, input_resolution=(8, 8), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0,modulator=None\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): WindowAttention(\n",
       "          dim=256, win_size=(8, 8), num_heads=16\n",
       "          (qkv): LinearProjection(\n",
       "            (to_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (to_kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): LeFF(\n",
       "          (linear1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (dwconv): Sequential(\n",
       "            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (linear2): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (eca): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): LeWinTransformerBlock(\n",
       "        dim=256, input_resolution=(8, 8), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0,modulator=None\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): WindowAttention(\n",
       "          dim=256, win_size=(8, 8), num_heads=16\n",
       "          (qkv): LinearProjection(\n",
       "            (to_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (to_kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): LeFF(\n",
       "          (linear1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (dwconv): Sequential(\n",
       "            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (linear2): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (eca): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (upsample_0): Upsample(\n",
       "    (deconv): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "  )\n",
       "  (decoderlayer_0): BasicUformerLayer(\n",
       "    dim=256, input_resolution=(16, 16), depth=2\n",
       "    (blocks): ModuleList(\n",
       "      (0): LeWinTransformerBlock(\n",
       "        dim=256, input_resolution=(16, 16), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0,modulator=Embedding(64, 256)\n",
       "        (modulator): Embedding(64, 256)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): WindowAttention(\n",
       "          dim=256, win_size=(8, 8), num_heads=16\n",
       "          (qkv): LinearProjection(\n",
       "            (to_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (to_kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): LeFF(\n",
       "          (linear1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (dwconv): Sequential(\n",
       "            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (linear2): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (eca): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): LeWinTransformerBlock(\n",
       "        dim=256, input_resolution=(16, 16), num_heads=16, win_size=8, shift_size=4, mlp_ratio=4.0,modulator=Embedding(64, 256)\n",
       "        (modulator): Embedding(64, 256)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): WindowAttention(\n",
       "          dim=256, win_size=(8, 8), num_heads=16\n",
       "          (qkv): LinearProjection(\n",
       "            (to_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (to_kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): LeFF(\n",
       "          (linear1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (dwconv): Sequential(\n",
       "            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (linear2): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (eca): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (upsample_1): Upsample(\n",
       "    (deconv): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "  )\n",
       "  (decoderlayer_1): BasicUformerLayer(\n",
       "    dim=128, input_resolution=(32, 32), depth=2\n",
       "    (blocks): ModuleList(\n",
       "      (0): LeWinTransformerBlock(\n",
       "        dim=128, input_resolution=(32, 32), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0,modulator=Embedding(64, 128)\n",
       "        (modulator): Embedding(64, 128)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): WindowAttention(\n",
       "          dim=128, win_size=(8, 8), num_heads=8\n",
       "          (qkv): LinearProjection(\n",
       "            (to_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (to_kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): LeFF(\n",
       "          (linear1): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (dwconv): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (linear2): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (eca): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): LeWinTransformerBlock(\n",
       "        dim=128, input_resolution=(32, 32), num_heads=8, win_size=8, shift_size=4, mlp_ratio=4.0,modulator=Embedding(64, 128)\n",
       "        (modulator): Embedding(64, 128)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): WindowAttention(\n",
       "          dim=128, win_size=(8, 8), num_heads=8\n",
       "          (qkv): LinearProjection(\n",
       "            (to_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (to_kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): LeFF(\n",
       "          (linear1): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (dwconv): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (linear2): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (eca): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (upsample_2): Upsample(\n",
       "    (deconv): Sequential(\n",
       "      (0): ConvTranspose2d(128, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "  )\n",
       "  (decoderlayer_2): BasicUformerLayer(\n",
       "    dim=64, input_resolution=(64, 64), depth=2\n",
       "    (blocks): ModuleList(\n",
       "      (0): LeWinTransformerBlock(\n",
       "        dim=64, input_resolution=(64, 64), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0,modulator=Embedding(64, 64)\n",
       "        (modulator): Embedding(64, 64)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): WindowAttention(\n",
       "          dim=64, win_size=(8, 8), num_heads=4\n",
       "          (qkv): LinearProjection(\n",
       "            (to_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (to_kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "          )\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): LeFF(\n",
       "          (linear1): Sequential(\n",
       "            (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (dwconv): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (linear2): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "          )\n",
       "          (eca): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): LeWinTransformerBlock(\n",
       "        dim=64, input_resolution=(64, 64), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0,modulator=Embedding(64, 64)\n",
       "        (modulator): Embedding(64, 64)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): WindowAttention(\n",
       "          dim=64, win_size=(8, 8), num_heads=4\n",
       "          (qkv): LinearProjection(\n",
       "            (to_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (to_kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "          )\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): LeFF(\n",
       "          (linear1): Sequential(\n",
       "            (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (dwconv): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (linear2): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "          )\n",
       "          (eca): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (upsample_3): Upsample(\n",
       "    (deconv): Sequential(\n",
       "      (0): ConvTranspose2d(64, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "  )\n",
       "  (decoderlayer_3): BasicUformerLayer(\n",
       "    dim=32, input_resolution=(128, 128), depth=2\n",
       "    (blocks): ModuleList(\n",
       "      (0): LeWinTransformerBlock(\n",
       "        dim=32, input_resolution=(128, 128), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0,modulator=Embedding(64, 32)\n",
       "        (modulator): Embedding(64, 32)\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): WindowAttention(\n",
       "          dim=32, win_size=(8, 8), num_heads=2\n",
       "          (qkv): LinearProjection(\n",
       "            (to_q): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (to_kv): Linear(in_features=32, out_features=64, bias=True)\n",
       "          )\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): LeFF(\n",
       "          (linear1): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (dwconv): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (linear2): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "          (eca): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): LeWinTransformerBlock(\n",
       "        dim=32, input_resolution=(128, 128), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0,modulator=Embedding(64, 32)\n",
       "        (modulator): Embedding(64, 32)\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): WindowAttention(\n",
       "          dim=32, win_size=(8, 8), num_heads=2\n",
       "          (qkv): LinearProjection(\n",
       "            (to_q): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (to_kv): Linear(in_features=32, out_features=64, bias=True)\n",
       "          )\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): LeFF(\n",
       "          (linear1): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (dwconv): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            (1): GELU()\n",
       "          )\n",
       "          (linear2): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "          (eca): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_restoration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taming",
   "language": "python",
   "name": "taming"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
