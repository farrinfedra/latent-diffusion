{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, sys, glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'base_learning_rate': 5e-05, 'target': 'ldm.models.diffusion.ddpm.LatentDiffusion', 'params': {'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'image', 'cond_stage_key': 'caption', 'image_size': 32, 'channels': 4, 'cond_stage_trainable': True, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'unet_config': {'target': 'ldm.modules.diffusionmodules.openaimodel.UNetModel', 'params': {'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_heads': 8, 'use_spatial_transformer': True, 'transformer_depth': 1, 'context_dim': 1280, 'use_checkpoint': True, 'legacy': False}}, 'first_stage_config': {'target': 'ldm.models.autoencoder.AutoencoderKL', 'params': {'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}}, 'cond_stage_config': {'target': 'ldm.modules.encoders.modules.BERTEmbedder', 'params': {'n_embed': 1280, 'n_layer': 32}}}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from models/ldm/text2img-large/model.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 872.30 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "config = OmegaConf.load(\"configs/latent-diffusion/txt2img-1p4B-eval.yaml\")\n",
    "model = load_model_from_config(\n",
    "config, \"models/ldm/text2img-large/model.ckpt\")  # TODO: check path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plms = None\n",
    "outdir = 'outputs/txt2img-samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plms:\n",
    "    sampler = PLMSSampler(model)\n",
    "else:\n",
    "    sampler = DDIMSampler(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = \"outputs/txt2img-samples\"\n",
    "prompt = \"a painting of a virus monster playing guitar\"\n",
    "H = 256\n",
    "W = 256\n",
    "n_samples = 4\n",
    "scale = 1.0\n",
    "ddim_eta = 0.0\n",
    "n_iter = 4\n",
    "ddim_steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 32, 32), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:27<00:00,  1.80it/s]\n",
      "Sampling:  25%|██▌       | 1/4 [00:29<01:27, 29.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 32, 32), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:27<00:00,  1.80it/s]\n",
      "Sampling:  50%|█████     | 2/4 [00:57<00:57, 28.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 32, 32), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:28<00:00,  1.78it/s]\n",
      "Sampling:  75%|███████▌  | 3/4 [01:26<00:28, 28.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 32, 32), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:28<00:00,  1.76it/s]\n",
      "Sampling: 100%|██████████| 4/4 [01:56<00:00, 29.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your samples are ready and waiting four you here: \n",
      "outputs/txt2img-samples \n",
      "Enjoy.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "outpath = outdir\n",
    "\n",
    "prompt = prompt\n",
    "\n",
    "sample_path = os.path.join(outpath, \"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "\n",
    "all_samples = list()\n",
    "with torch.no_grad():\n",
    "    with model.ema_scope():\n",
    "        uc = None\n",
    "        if scale != 1.0:\n",
    "            uc = model.get_learned_conditioning(n_samples * [\"\"])\n",
    "        for n in trange(n_iter, desc=\"Sampling\"):\n",
    "            c = model.get_learned_conditioning(n_samples * [prompt])\n",
    "            shape = [4, H//8, W//8]\n",
    "            samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                            conditioning=c,\n",
    "                                            batch_size=n_samples,\n",
    "                                            shape=shape,\n",
    "                                            verbose=False,\n",
    "                                            unconditional_guidance_scale=scale,\n",
    "                                            unconditional_conditioning=uc,\n",
    "                                            eta=ddim_eta)\n",
    "\n",
    "            x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "            x_samples_ddim = torch.clamp(\n",
    "                (x_samples_ddim+1.0)/2.0, min=0.0, max=1.0)\n",
    "\n",
    "            for x_sample in x_samples_ddim:\n",
    "                x_sample = 255. * \\\n",
    "                    rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "                    os.path.join(sample_path, f\"{base_count:04}.png\"))\n",
    "                base_count += 1\n",
    "            all_samples.append(x_samples_ddim)\n",
    "\n",
    "# additionally, save as grid\n",
    "grid = torch.stack(all_samples, 0)\n",
    "grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "grid = make_grid(grid, nrow=n_samples)\n",
    "\n",
    "# to image\n",
    "grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "Image.fromarray(grid.astype(np.uint8)).save(\n",
    "    os.path.join(outpath, f'{prompt.replace(\" \", \"-\")}.png'))\n",
    "\n",
    "print(\n",
    "f\"Your samples are ready and waiting four you here: \\n{outpath} \\nEnjoy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "7596f27fb000ef2d8640d8938f263877bcfa2628de186188493dd8d6d4f16fe5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
